# Pipeline ETL: Extra√ß√£o de API ‚Üí Transforma√ß√£o com Pandas ‚Üí Carga em MySQL

## üß© Descri√ß√£o

Este projeto implementa uma **pipeline ETL (Extract, Transform, Load)** em **Python**, que executa o fluxo completo de ingest√£o de dados:

1. **Extra√ß√£o** de dados de uma **API REST**;
2. **Armazenamento** dos dados brutos em **arquivos CSV**;
3. **Transforma√ß√£o e limpeza** dos dados utilizando **Pandas**;
4. **Carga** dos dados tratados em um banco de dados **MySQL**.

A pipeline pode ser executada tanto por **scripts Python** quanto via **notebooks Jupyter**, o que facilita a explora√ß√£o interativa e o desenvolvimento incremental.

---

## üèóÔ∏è Estrutura do Projeto

```
pipeline_mongo_mysql/
‚îÇ
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ tabela_livros.csv
‚îÇ   ‚îî‚îÄ‚îÄ tabela_produtos_2021_2029.csv
‚îÇ
‚îú‚îÄ‚îÄ data_teste/
‚îÇ   ‚îú‚îÄ‚îÄ tb_livros.csv
‚îÇ   ‚îî‚îÄ‚îÄ tb_produtos.csv
‚îÇ
‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îú‚îÄ‚îÄ extract_and_save_data.py   # Extra√ß√£o dos dados da API e salvamento em CSV
‚îÇ   ‚îú‚îÄ‚îÄ transform_data.py          # Transforma√ß√£o e normaliza√ß√£o com Pandas
‚îÇ   ‚îî‚îÄ‚îÄ save_data_mysql.py         # Inser√ß√£o dos dados transformados no MySQL
‚îÇ
‚îú‚îÄ‚îÄ notebooks/
‚îÇ   ‚îú‚îÄ‚îÄ extract_and_save_data.ipynb
‚îÇ   ‚îú‚îÄ‚îÄ transform_data.ipynb
‚îÇ   ‚îî‚îÄ‚îÄ save_data_mysql.ipynb
‚îÇ
‚îú‚îÄ‚îÄ .env                           # Configura√ß√µes de ambiente (n√£o versionado)
‚îú‚îÄ‚îÄ requirements.txt               # Depend√™ncias do projeto
‚îî‚îÄ‚îÄ README.md
```

---

## ‚öôÔ∏è Instala√ß√£o

### 1Ô∏è‚É£ Clonar o reposit√≥rio
```bash
git clone https://github.com/raulblummertz/pipeline_mongo_mysql.git
cd pipeline_mongo_mysql
```

### 2Ô∏è‚É£ Criar e ativar um ambiente virtual
```bash
python3 -m venv .venv
source .venv/bin/activate
```

### 3Ô∏è‚É£ Instalar as depend√™ncias
```bash
pip install -r requirements.txt
```

---

## üîë Configura√ß√£o de Ambiente

Crie um arquivo **`.env`** na raiz do projeto com as vari√°veis de conex√£o e par√¢metros de acesso:

```bash
DB_PASSWORD="sua_senha_mongodb" # fique a vontade pra alterar para MONGO_PASSWORD e corrigir no c√≥digo tamb√©m
API_URL="https://labdados.com/produtos"
DB_HOST="localhost"
DB_USER="seu_usuario_mysql"
MYSQL_PASSWORD="sua_senha_mysql"
```

> ‚ö†Ô∏è **Aten√ß√£o:** o arquivo `.env` cont√©m informa√ß√µes sens√≠veis e **n√£o deve ser versionado**.  
> Adicione-o ao `.gitignore` para evitar exposi√ß√£o de credenciais.

---

## ‚ñ∂Ô∏è Execu√ß√£o

Voc√™ pode executar a pipeline completa ou cada etapa separadamente.

### 1Ô∏è‚É£ Extrair e salvar dados da API
```bash
python scripts/extract_and_save_data.py
```
Ou via notebook:
```
notebooks/extract_and_save_data.ipynb
```

### 2Ô∏è‚É£ Transformar e normalizar os dados
```bash
python scripts/transform_data.py
```
Ou via notebook:
```
notebooks/transform_data.ipynb
```

### 3Ô∏è‚É£ Inserir dados no banco MySQL
```bash
python scripts/save_data_mysql.py
```
Ou via notebook:
```
notebooks/save_data_mysql.ipynb
```

---

## üß† Tecnologias Utilizadas

- **Python 3.10+**
- **Requests** ‚Äî Requisi√ß√µes HTTP para a API
- **Pandas** ‚Äî Limpeza, transforma√ß√£o e normaliza√ß√£o dos dados
- **MySQL Connector (mysql-connector-python)** ‚Äî Conex√£o e inser√ß√£o no banco MySQL
- **dotenv** ‚Äî Carregamento de vari√°veis de ambiente

---

## üß™ Estrutura de Dados

- **`data/`**: Cont√©m os arquivos CSV gerados a partir da API.  
- **`data_teste/`**: Conjunto de dados reduzidos para testes r√°pidos.  
- **`scripts/`**: Scripts Python que executam cada etapa do pipeline.  
- **`notebooks/`**: Notebooks Jupyter com a mesma l√≥gica dos scripts, voltados para experimenta√ß√£o.

---

## üë§ Autor

**Raul Lummertz**  
üìé [github.com/raulblummertz](https://github.com/raulblummertz)

README criado por IA e revisado pra n√£o deixar falar bobagem
